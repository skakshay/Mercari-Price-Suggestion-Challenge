{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "6fef8047-c79a-4955-9f9b-2f6c87117d2f",
        "_uuid": "f6250c09f51b7f2be8093bc51287a094f046b53b",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nfrom subprocess import check_output\n# print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n\nimport random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom scipy import stats\nfrom scipy.special import inv_boxcox\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom xgboost import XGBRegressor\n\nimport seaborn as sns\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "fcf90446-5d6e-454f-949c-0974a400abe9",
        "_uuid": "398e45f607f07dcac8d1e13e0069b19152d8a524",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "train = pd.read_csv(\"../input/train.tsv\", sep='\\t', index_col=0)\ntest = pd.read_csv(\"../input/test.tsv\", sep='\\t', index_col=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c5d5910e-71b3-444e-89c8-82b9792291db",
        "_uuid": "e5134115b72206fb955121f0c54f2452c2e177df",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "train.isnull().sum(), test.isnull().sum()\ntrain.shape, test.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4424a8dc-34a4-48ec-a7cc-c7e4eda3213e",
        "_uuid": "1a04a496c840ecb33ea185bb7918717f7ba4abed",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "train.price.describe()\n_,ax = plt.subplots(figsize=(6,6))\nsns.distplot(train.price, hist=False, ax=ax)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1e9941bd-bf58-4f72-915d-b88c1433f5e6",
        "_uuid": "5cba62dda3e76e6384a6f7cf6e1a2acac5ae7deb"
      },
      "cell_type": "markdown",
      "source": "The distribution of 'price' is highly skewed, so we might have to log transform this for applying regression models. If we use tree based models we can skip this transformation as tree models are quite robust to skewness and outliers. "
    },
    {
      "metadata": {
        "_cell_guid": "c45a8c79-ed16-4672-9047-712177effb20",
        "_uuid": "fbd35935067ee6070bb10ae33173a91b962c8226",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "_,ax = plt.subplots(figsize=(6,6))\n_ = stats.probplot(train.price, dist=stats.norm, plot=ax)\nprint (\"Skewness : {0}\".format(stats.skew(train.price)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "64092507-aaf2-4a54-b863-e891a7483886",
        "_uuid": "d74bf4706be9b508d5ba4cf2af4435973411a621"
      },
      "cell_type": "markdown",
      "source": "The distribution of price is highly skewed towards positive side. We will transform our price using stats.boxcox . We add 1 to the price to accumulate items with 0 price."
    },
    {
      "metadata": {
        "_cell_guid": "d09ce717-66f5-4f7a-9f24-5fac4e1ad9fc",
        "_uuid": "78391419f0d480d507f40d3f3d41dba21e32e77e",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "price_box, lmbda = stats.boxcox(train['price']+1)\nprint (\"Lambda : {0}\".format(lmbda))\n_,ax = plt.subplots(figsize=(5,5))\n_ = stats.probplot(price_box, dist=stats.norm, plot=ax)\nprint (\"Skewness : {0}\".format(stats.skew(price_box)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d7ca32e8-3de4-4927-8573-45fac5be6bdc",
        "_uuid": "30f966b02fe0b902c70133cf5b19fc7fdfa618ab"
      },
      "cell_type": "markdown",
      "source": "Ohh here we have a negative lambda, which might create troubles during our inverse coxbox transformation. So lets try square root or logarithmic transformations and see which turns out to be better. \n"
    },
    {
      "metadata": {
        "_cell_guid": "337123c6-2b9c-40ea-9700-9b062ab2681a",
        "_uuid": "82e2e24cc943446745462e64e9e29a4119788373",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "price_log = np.log(train['price']+1)\n_,ax = plt.subplots(figsize=(5,5))\n_ = stats.probplot(price_log, dist=stats.norm, plot=ax)\nprint (\"Skewness : {0}\".format(stats.skew(price_log)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "6a912acd-6896-451a-973a-4bde14c144a7",
        "_uuid": "447db5c1496458855869444b20de5d88bd985761",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "price_sqrt = np.sqrt(train['price'])\n_,ax = plt.subplots(figsize=(5,5))\n_ = stats.probplot(price_sqrt, dist=stats.norm, plot=ax)\nprint (\"Skewness : {0}\".format(stats.skew(price_sqrt)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "754da691-e15d-4472-adf9-b9def13c581c",
        "_uuid": "2c2619e3fcce99401f4506a80fc8fc1304b6871a"
      },
      "cell_type": "markdown",
      "source": "Logarithmic transformation has less skewness and thus closer to normal."
    },
    {
      "metadata": {
        "_cell_guid": "64bdc404-1ff8-4a42-8bd1-8997ccc49964",
        "_uuid": "bdb3f0b684ba4d890726fd8ccdb8608e60d9313a",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "train['price'] = np.log1p(train['price'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4edc741b-a444-4c39-a9d7-9c6973f841f6",
        "_uuid": "40f1bb940d67d6d862df0a98ad466288c8f85c52",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "all_data = pd.concat([train.drop(['price'], axis=1), test])\nall_data.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b8499f7e-137f-4e06-92f9-1b2f5215526a",
        "_uuid": "ff3116a0862f8b058ef6c58fbb9b4a7ba7433f03",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "def cat_no(category) : \n    try : return len([s.strip() for s in category.split('/')])\n    except : return 0\n\nall_data['cat_no'] = all_data.category_name.apply(cat_no)\nall_data.cat_no.value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c6b1a80d-5aee-4700-8b1a-331419079dd5",
        "_uuid": "9704802fe5e5ec7b5abeb5007f018ba98afc3f14"
      },
      "cell_type": "markdown",
      "source": "In category_name as we see there are  multiple subcategories which can be 3,4 or 5. There are null values as well. for which cat_no is 0. So we create different features for each subcategory. "
    },
    {
      "metadata": {
        "_cell_guid": "8dd1970d-4688-473f-bfb0-b8b0997c74e4",
        "_uuid": "c8cbd85bfe1de7dabbca1f5d3d534a6138f03866",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "def split_cat(category) : \n    try: \n        cat_no = len([s.strip() for s in category.split('/')])\n        if (cat_no==3):\n            return category.split('/')+['NA', 'NA']\n        elif (cat_no==4):\n            return category.split('/')+['NA']\n        elif (cat_no==5):\n            return category.split('/')\n    except : \n        return ['NA', 'NA', 'NA', 'NA', 'NA']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c8c2d67c-645c-4d24-8077-ecb1e126ae80",
        "_uuid": "bf944cbfc8cb3acae71f7bb20a7a7f32ed2ee9ad",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "all_data['cat1'], all_data['cat2'], all_data['cat3'], all_data['cat4'], all_data['cat5'] = \\\nzip(*all_data['category_name'].apply(split_cat))\nall_data.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "7c28eda9-5706-40b0-b3b7-844cc581dd2d",
        "_uuid": "15088d0b60466e0944773dd857f7d20c7ea0b3ff",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "all_data.loc[all_data.cat4=='NA'].shape[0]/all_data.shape[0], all_data.loc[all_data.cat5=='NA'].shape[0]/all_data.shape[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f4beafc9-9fe1-4d37-90b6-9c0f84198dd2",
        "_uuid": "491a7cfe9fed6faa9f4197e59edf166adff4676f"
      },
      "cell_type": "markdown",
      "source": "More than 99% values in cat4 and cat5 are 'NA', so we can drop these 2 from our model. \nNow we can map our categorical columns to numbers. "
    },
    {
      "metadata": {
        "_cell_guid": "8658803d-ccd9-45b7-9b1f-d9a19fdbafe9",
        "_uuid": "59224ae1a3e9a0e89e00a4e03682bf0af7b6e0ad",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "all_data['cat1'] = all_data['cat1'].astype('category').cat.codes\nall_data['cat2'] = all_data['cat2'].astype('category').cat.codes\nall_data['cat3'] = all_data['cat3'].astype('category').cat.codes\nall_data['cat4'] = all_data['cat4'].astype('category').cat.codes\nall_data['cat5'] = all_data['cat5'].astype('category').cat.codes",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d24aef07-8d2c-4b50-b41f-dd04e931f1e4",
        "_uuid": "830afefd3de59b31fceb41d57e6b034f1907ccd0",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "all_data.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c95aef8c-e839-45ef-98c3-fa4e7526717e",
        "_uuid": "fb7122df7dc380a7557f3058c99a8d05fbde5f0a"
      },
      "cell_type": "markdown",
      "source": "Getting back our train and test from all_data"
    },
    {
      "metadata": {
        "_cell_guid": "0972d9d5-ddc2-401b-ab41-d7f545c59222",
        "scrolled": true,
        "_uuid": "e6907987ddae539cb50465e9b075c849ac2722e4",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "train_df = all_data[:train.shape[0]]\ntrain_df['price'] = train.price\ntest_df = all_data[train.shape[0]:]\ntrain_df.shape, test_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "fee12d9c-a371-47cd-adec-10fea0653866",
        "_uuid": "7e50e1178077aa6dbe786cec8d2e0341650fa8af",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "train.isnull().sum(), test.isnull().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "9e3c9670-0949-430a-8809-bc8e8e33af9b",
        "_uuid": "9aaf77aa777f522902e02e96f1e1c479e0e9e925"
      },
      "cell_type": "markdown",
      "source": "In train we have null values in category_name, brand_name and item_description. In test there are null values in brand_name and category_name. \n1. We will fill null values only in brand_name feature. \n2. For null in category_name we already have cat1, cat2 cat3 as 'NA'\n3. We drop item_description from our model so no need to fill null here. \n"
    },
    {
      "metadata": {
        "_cell_guid": "ef9bab71-4dea-4c74-9217-a450a7e923b8",
        "_uuid": "c5193f4b6fae886cf55bb7b6708b23914bb7e271",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "train_df.loc[:, ['brand_name']], test_df.loc[:,['brand_name']] = (df.loc[:,['brand_name']].fillna('NA') for df in [train_df, test_df])\ntrain_df['brand_name'] = train_df['brand_name'].astype('category').cat.codes\ntest_df['brand_name'] = test_df['brand_name'].astype('category').cat.codes",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5644f2f3-7f58-4789-afa5-1ac77465d4c4",
        "_uuid": "113159c4ede21fb2d86c08a85c8d6ec69be27066"
      },
      "cell_type": "markdown",
      "source": "Now a little bit of intuition can help to identify that linear regression won't be of help here. So we experiment with tree based models as the data can be divided into various regions based on various predictors(first on category, then on brand, shipping, condition etc) to get a good prediction. Also we chose to ignore the 'name'( as it is represented by 'category' and 'brand') and item_description(which is highly subjective and need a NLP approach to be used).\n\nAlso we are using original train without adjusting for skewness as tree based models are quite robust to skewness and outliers."
    },
    {
      "metadata": {
        "_cell_guid": "13d9cf28-7d5c-4307-b5b6-27feabed3ef1",
        "_uuid": "f79a020c4b2170b4d3140d7d77e8ff963ff20a32",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "predictors = ['item_condition_id', 'brand_name', 'shipping', 'cat_no', 'cat1', 'cat2', 'cat3']\nX = train_df[predictors]\ny = train_df.price\n_=sns.distplot(train_df.price, hist=False)\ntrain_df.price.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a9d910a1-173b-421b-97b7-6f71d0755f4e",
        "_uuid": "86d36504d047c1306e020655624a2bddb79b14e0"
      },
      "cell_type": "markdown",
      "source": "Our training set contains 1.4 million samples and thus tuning our gradient boosting model will take a logn time. So let's downsample our train_df so that training and tuning are fast and when all parameters are decided we can train on full model. "
    },
    {
      "metadata": {
        "_cell_guid": "6e855b67-8d2f-4c8f-92d5-46fa38f35d1c",
        "_uuid": "5081b9baf5e2facf1878f63ef1cd425a4b869e79",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "small_train = train_df.sample(frac=0.1, random_state=11)\nsmall_train.shape\nsmall_train.price.describe()\n_=sns.distplot(small_train.price, hist=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "22f4d7c5-2671-443c-9f2a-c647a367549c",
        "_uuid": "476648b6d023afc353aa5f5e2104a90bdf86dc67"
      },
      "cell_type": "markdown",
      "source": "The sample we have taken has almost same stats for price feature and a similar distribuion. "
    },
    {
      "metadata": {
        "_cell_guid": "21624bad-1a1c-425f-b452-aeb2ef32a675",
        "_uuid": "048c3245ec0e8a10145cc598775d6c1fe6a4aec0",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Root Mean square Logarithmic error\ndef rmsle(y_true, y_pred):\n    return np.sqrt(np.square( np.log(y_pred+1)- np.log(y_true+1) ).sum() / y_true.shape[0])\n\n# Root mean square logarithmic error\ndef scorer(estimator, X, y):\n    estimator.fit(X, y)\n    y_pred = estimator.predict(X)\n    return -1*np.sqrt(np.square( np.log(y_pred+1)- np.log(y+1) ).sum() / y.shape[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "192ee379-941f-4044-b41f-4f021c94c273",
        "_uuid": "489f630a4939601ac0217cafa756019221cd9ee7",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "def model_fit_evaluate(estimator, train, predictors, performCV=True, featureImportances=True, cv_folds=5):\n    # fit the estimator on train\n    estimator.fit(train[predictors], train['price'])\n    \n    # predict train\n    train_pred = estimator.predict(train[predictors])\n    \n    # perform CV\n    if performCV : \n        cv_score = cross_val_score(estimator, train[predictors], train['price'], cv=cv_folds, scoring=scorer)\n    \n    # print RMSLE cv score : \n    print (\"Mean : {0}\\nStd : {1}\".format(cv_score.mean(), cv_score.std()))\n\n    #feature importances\n    if featureImportances : \n        feat_imp = pd.Series(estimator.feature_importances_, predictors).sort_values(ascending=False)\n        feat_imp.plot(kind='bar', title='Feature Importance')\n        plt.ylabel('Feature imortance score')\n    \n    \n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "075a9dc8-bd91-44e4-a52d-1d1a09c5ca6b",
        "_uuid": "cd6398e590f6cfc066c920d50a28734b8b369b42"
      },
      "cell_type": "markdown",
      "source": "Let's first create a baseline model with default parameters."
    },
    {
      "metadata": {
        "_cell_guid": "cba35eb8-a72e-41a8-8549-4934c9427257",
        "scrolled": true,
        "_uuid": "4e83672268a4440288f6d0e80c3637e1e3ab43b5",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "gbr0 = GradientBoostingRegressor(random_state=11, verbose=1)\npredictors = ['item_condition_id', 'brand_name', 'shipping', 'cat_no', 'cat1', 'cat2', 'cat3']\nmodel_fit_evaluate(gbr0, small_train, predictors)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d6f4f6ef-537b-4966-94fa-b38e42295f23",
        "_uuid": "9f216339ab939575997b3dc79987fc3fe1714bf0"
      },
      "cell_type": "markdown",
      "source": "This is the RMSLE for a base model and we will try to get better reults by tuning parameters. "
    },
    {
      "metadata": {
        "_cell_guid": "1a14cbe6-8d23-47e1-9545-bef3f390bd75",
        "_uuid": "c00c104f641f616f6de920422627060ba818af4a",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "params1 = {'n_estimators':range(20,81,10)}\ngbr1 = GradientBoostingRegressor(learning_rate=0.2, subsample=0.8, min_samples_split=2000, \n                                 min_samples_leaf=100, max_depth=20, random_state=11, \n                                 verbose=1, max_features='sqrt')\ngsearch1 = GridSearchCV(gbr1, params1, scoring=scorer, cv=5, verbose=1, return_train_score=True)\ngsearch1.fit(small_train[predictors], small_train.price)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "2fb30f4f-0eec-45df-8273-b379c09f4dda",
        "_uuid": "87c045a7387a1cbc69b815a00d631bdeedc23772",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "gsearch1.cv_results_['mean_test_score']\npd.DataFrame(gsearch1.cv_results_['mean_test_score'], index=range(20, 81, 10))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d0d9fbbf-5263-48cf-805c-1c38f26bb513",
        "_uuid": "7808fc902d8d5c1d308c8d8235da537596865d66"
      },
      "cell_type": "markdown",
      "source": "As we see when n_estimators is 80 we have maximum cv_score or minimum RMLSE. It will be better if we try to increase learning rate and look for an optimum value of n_estimators in this range(ie under 100), but for now lets continue with 80. "
    },
    {
      "metadata": {
        "_cell_guid": "661d8dbd-1288-4d9e-96e0-85cca6683cd0",
        "_uuid": "ae66996f249fdea99b4d4c1db740e486824ef90f",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "params2 = {'n_estimators':range(100,301,50)}\ngbr2 = GradientBoostingRegressor(learning_rate=1.5, subsample=0.8, min_samples_split=1500, \n                                 min_samples_leaf=100, max_depth=2, random_state=11, \n                                 verbose=1, max_features='sqrt' )\ngsearch2 = GridSearchCV(gbr2, params2, cv=5, verbose=1, return_train_score=True)\ngsearch2.fit(small_train[predictors], small_train.price)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "45773853-900d-4e2d-a3ef-4b469aeee24e",
        "scrolled": true,
        "_uuid": "48dcb0a5ce71a1f79f4a5f62ee45c6de15fa359a",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "params3 = {'max_depth':range(5,16,2), 'min_samples_split':range(200,1001,200)}\ngbr3 = GradientBoostingRegressor(n_estimators=80, subsample=0.8, \n                                 min_samples_leaf=100, random_state=11, \n                                 verbose=1, max_features='sqrt' )\ngsearch3 = GridSearchCV(gbr3, params3, cv=5, verbose=1, return_train_score=True)\ngsearch3.fit(small_train[predictors], small_train.price)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a2342fa5-20b6-48a7-9be0-e9ae55e11408",
        "_uuid": "45114cba3c98560e9632e83c21bf330dbef6ae06",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Root Mean square Logarithmic error\ndef rmsle(y_true, y_pred):\n    return np.sqrt(np.square( np.log(y_pred+1)- np.log(y_true+1) ).sum() / y_true.shape[0])\n\n# Root mean square logarithmic error\ndef scorer(estimator, X, y):\n    estimator.fit(X, y)\n    y_pred = estimator.predict(X)\n    return -1*np.sqrt(np.square( np.log(y_pred+1)- np.log(y+1) ).sum() / y.shape[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ae5731c9-d03e-4bc6-bae1-30df40618b64",
        "_uuid": "d4c0909497b0490d5142c576d2969142f085ea4f",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "predictors = ['item_condition_id', 'brand_name', 'shipping', 'cat_no', 'cat1', 'cat2', 'cat3']\nX = train_df[predictors]\ny = train_df.price\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=11, shuffle=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ed725d5d-b92c-438b-b958-2692e5efaf80",
        "scrolled": true,
        "_uuid": "90a41afb5be885cb0746aa4aeb5517713f63355d",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "rfr0 = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=11, verbose=1, max_depth=10)\nscore = cross_val_score(rfr0, X, y, scoring=scorer, verbose=1, cv=5)\nscore.mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "9d2fd3aa-b347-4c3a-949e-827bd8559fe7",
        "_uuid": "20bc8337c64c7b5a31c4698b4eac4d76d8262477",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "After trying various values of max_depth and n_estimators, we found that the best rmsle is obtained by n_estimators=100 and max_depth = None. So now lets just try to increase n_estimators so improve score. We won;t use cross_val_score as it tkes lot of time, instead check score on a test set. "
    },
    {
      "metadata": {
        "_cell_guid": "1a769678-a173-4e7d-b8bf-8869238d7964",
        "scrolled": true,
        "_uuid": "3faaf8cfe943f4f697f374a93901c7890e92c645",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "rfr1 = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=11, verbose=2, max_depth=20)\nrfr1.fit(X_train, y_train)\ny_pred = rfr1.predict(X_test)\nrmsl_error = rmsle(y_test, y_pred)\nprint (\"RMSLE : {0}\".format(rmsl_error))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "774a611f-bc2b-4eec-aa79-87f3129b92fd",
        "_uuid": "39b529699533de45050b6acfbd5a045323d12c36"
      },
      "cell_type": "markdown",
      "source": "Increasing n_estimators doesn;t give better results but changing max_depth slightly improves rmlse. So now lets train our random forest on full data with these parameters and submit the result. "
    },
    {
      "metadata": {
        "_cell_guid": "4c4855f1-344a-4c45-a279-3e6436e72d96",
        "scrolled": true,
        "_uuid": "95a1bf3081f887a5861a9aa4853c06ee62e33863",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "rfr2 = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=11, verbose=2, max_depth=20)\nrfr2.fit(X, y)\ny_pred = rfr2.predict(test_df[predictors])\ny_pred = inv_boxcox(y_pred, lmbda) - 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1db7c1dc-ca01-45a2-af13-3a729025f60b",
        "_uuid": "4b10773ded57cf1ef4e6a0b22c79126633f0c4e6",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "sub = pd.DataFrame(y_pred, index=test.index, columns=['price'])\nsub.to_csv('sub_rfr2.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5cba7760-3d4d-47b7-9833-6148cebf9062",
        "_uuid": "57aebe213f20cc9ac194e35b675779ae7a4f577d"
      },
      "cell_type": "markdown",
      "source": "Now we use Gradient Boosting model to preidct prices. We use smaller n_estimators to reduce trainig time, and also since n_estimators is correlated with learning rate which we are chossing using cross validation. "
    },
    {
      "metadata": {
        "_cell_guid": "d0c71274-39f4-43f2-9d5f-9b4b22139f5b",
        "scrolled": true,
        "_uuid": "8e2e1948227d69fbcd1439b6ae0ac048721801da",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "gbr = GradientBoostingRegressor(learning_rate=1, n_estimators=500, max_depth=8, subsample=0.8, random_state=11, verbose=1, max_features='sqrt')\ngbr.fit(X_train, y_train)\ny_pred = gbr.predict(X_test)\ny_pred = inv_boxcox(y_pred, lmbda) - 1\ny_true = inv_boxcox(y_test, lmbda) - 1\nrmsl_error = rmsle(y_true, y_pred)\nprint (\"RMSLE : {0}\".format(rmsl_error))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "fcf3ea32-119b-45dc-be2c-34ed2d0be1ac",
        "scrolled": false,
        "_uuid": "d9fa40bf79928ffc1224423f4a1bc994f249d8af",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "gbr = GradientBoostingRegressor(learning_rate=1, n_estimators=300, max_depth=8, max_features='sqrt', subsample=0.8, random_state=11, verbose=1)\ngbr.fit(X, y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5febc51c-1e28-4d82-b4c6-772bb13f1dd1",
        "_uuid": "a3d3dbd491fa365c705cddc98779aa2386bffb2b",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "y_pred = gbr.predict(test_df[predictors])\ny_pred = np.expm1(y_pred)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "cd939f63-9c9e-4efb-b668-130529453bfd",
        "_uuid": "f7b400f42705f54ac57dce6547461f76ff64c5d5",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "sub = pd.DataFrame(y_pred, index=test.index, columns=['price'])\nsub.loc[sub['price']<0]=0\nsub.to_csv('sub_gbr2.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "3560cbeb-cac4-48a5-a9cf-8ff6e6139e05",
        "scrolled": false,
        "_uuid": "29089214eb113c4f4865c7911bbece55a6dfc35a",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "gbr = GradientBoostingRegressor(max_features='sqrt', n_estimators=500, learning_rate=0.1, verbose=1)\ngbr.fit(X_train, y_train)\ny_pred = gbr.predict(X_test)\ny_pred = inv_boxcox(y_pred, lmbda) - 1\nrmsl_error = rmsle(inv_boxcox(y_test, lmbda) - 1, y_pred)\nprint (\"Error  : {0}\".format(rmsl_error))\n\n\ny_pred = gbr.predict(test[['item_condition_id', 'category_no', 'brand_no', 'shipping']])\ny_pred = inv_boxcox(y_pred, lmbda) - 1",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "33114176-b8ee-4883-8488-38e195d74c3a",
        "_uuid": "35aebd6f849a3eef5e58c443032d46a8dd2b0a19",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "sub = pd.DataFrame(y_pred, index=test.index, columns=['price'])\nsub.to_csv('sub_gbr.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "989f43f2-6d88-4183-b71a-b5bda47eb3d0",
        "_uuid": "c19fa2eaedfae39d3d63babe7d7c712d48ee2bc2",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}